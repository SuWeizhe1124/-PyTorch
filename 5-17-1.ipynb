{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SuWeizhe1124/-PyTorch/blob/master/5-17-1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5fCEDCU_qrC0"
      },
      "source": [
        "## AlphaGo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQoxjCeDBl-4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "8acbb5f9-962b-4e1f-cdab-e8845e92ebc0"
      },
      "source": [
        "from torch.nn import Linear, ReLU, Sequential\n",
        "net = Sequential(\n",
        "        Linear(3, 8), # 第1层有8个神经元\n",
        "        ReLU(), # 第1层神经元的非线性函数是max(*,0)\n",
        "        Linear(8, 8), # 第2层有8个神经元\n",
        "        ReLU(), # 第2层的神经元的非线性函数是max(*,0)\n",
        "        Linear(8, 1), # 第3层有1个神经元\n",
        "        )\n",
        "def g(x, y):\n",
        "    x0, x1, x2 = x[:, 0] ** 0, x[:, 1] ** 1, x[:, 2] ** 2\n",
        "    y0 = y[:, 0]\n",
        "    return (x0 + x1 + x2) * y0 - y0 * y0 - x0 * x1 * x2\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "\n",
        "optimizer = Adam(net.parameters())\n",
        "for step in range(1000):\n",
        "    optimizer.zero_grad()\n",
        "    x = torch.randn(1000, 3)\n",
        "    y = net(x)\n",
        "    outputs = g(x, y)\n",
        "    loss = -torch.sum(outputs)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if step % 100 == 0:\n",
        "        print ('iteration #{}: loss = {}'.format(step, loss.item()))\n",
        "x_test = torch.randn(2, 3)\n",
        "print ('测试输入: {}'.format(x_test))\n",
        "\n",
        "# 查看神经网络的计算结果\n",
        "y_test = net(x_test)\n",
        "print ('人工神经网络计算结果: {}'.format(y_test))\n",
        "print ('g的值: {}'.format(g(x_test, y_test)))\n",
        "\n",
        "# 根据理论计算参考答案\n",
        "def argmax_g(x):\n",
        "    x0, x1, x2 = x[:, 0] ** 0, x[:, 1] ** 1, x[:, 2] ** 2\n",
        "    return 0.5 * (x0 + x1 + x2)[:, None]\n",
        "yref_test = argmax_g(x_test)\n",
        "print ('理论最优值: {}'.format(yref_test))\n",
        "print ('g的值: {}'.format(g(x_test, yref_test)))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration #0: loss = 130.5523681640625\n",
            "iteration #100: loss = -664.4542236328125\n",
            "iteration #200: loss = -1342.6414794921875\n",
            "iteration #300: loss = -1646.7431640625\n",
            "iteration #400: loss = -1618.9427490234375\n",
            "iteration #500: loss = -1762.1075439453125\n",
            "iteration #600: loss = -1607.3470458984375\n",
            "iteration #700: loss = -1713.916015625\n",
            "iteration #800: loss = -1607.0050048828125\n",
            "iteration #900: loss = -1623.4312744140625\n",
            "测试输入: tensor([[-0.1606,  0.8650, -0.1625],\n",
            "        [ 0.2697,  0.6126,  0.1407]])\n",
            "人工神经网络计算结果: tensor([[0.8961],\n",
            "        [0.7284]], grad_fn=<AddmmBackward>)\n",
            "g的值: tensor([0.8691, 0.6463], grad_fn=<SubBackward0>)\n",
            "理论最优值: tensor([[0.9457],\n",
            "        [0.8162]])\n",
            "g的值: tensor([0.8715, 0.6540])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}